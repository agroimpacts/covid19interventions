---
title: "Google Mobility Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{mobility_google}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo=FALSE, include=FALSE}
library(covid19interventions)
library(rvest)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(readr)
```

- Find date of biggest drop in mobility by county using google data
- Check if this happens before or after SAH order is implemented
- If a big drop before SAH -> there may be some county level implementation (?)
```{r, fig.align='center', fig.width=8, fig.height=5}
# global dataset
mob_data <- read_csv("data/Global_Mobility_Report.csv")

# mobility change in NY County by type
mob_data %>% 
  filter(sub_region_1 == "New York" & sub_region_2 == "New York County") %>%
  select("date","retail_and_recreation_percent_change_from_baseline",
        "grocery_and_pharmacy_percent_change_from_baseline",
        "parks_percent_change_from_baseline",
        "transit_stations_percent_change_from_baseline",
        "workplaces_percent_change_from_baseline",
        "residential_percent_change_from_baseline")%>%
  gather(key = "variable", value = "value", -date) %>% 
  mutate(type = str_replace(variable, "_percent_change_from_baseline", ""),
         date = as.Date(date, format = '%m/%d/%Y'))%>%
  ggplot(aes(y = value,x = date, group= type, color = type))+
  geom_line() +
  ggtitle("Percent Change From Baseline")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90), 
        legend.title = element_blank(),
        legend.position="bottom")+
  scale_x_date(date_minor_breaks = "1 day")
```

# State/County Stay at Home Dates from NYTimes
```{r}
# website with stay at home/shelter in place orders
url <- xml2::read_html("https://www.nytimes.com/interactive/2020/us/coronavirus-stay-at-home-order.html")

# identify elements to scrape at the state level
td_tags <- c('h3', 'p.l-order')
sah_ = list()

# loop through multiple elements to scrape 
for (i in td_tags){
  sah_[[i]] = url %>%
  html_nodes('div.state-wrap.statewide')  %>%
  html_nodes(i) %>% 
  html_text
}

# list to Dataframe conversion
names(sah_) <- c('state','order')
sah <- as.data.frame(sah_)
sah["county"] <- 'All'

# States that did not implement a stay at home order
# Pull the county level info with state attached
no_sah <- html_nodes(url, "div.state-wrap ")
for (section in no_sah) {
  state <- html_nodes(section, "h3") %>% html_text
  place <- html_nodes(section, "p.l-place") %>% html_text
  order <- html_nodes(section, "p.l-order") %>% html_text
  # counties under l-place class for every state
  # if state has implemented SAH in one or more county
  # extract county level SAH data
  if (length(place) >= 1){
    print(state)
    print(unlist(order))
    sah <- sah %>% add_row(state = state, order = order, county = place)
  }
}

# Format state Column
sah$state = unlist(lapply(sah$state,function(x){
  paste(unlist(strsplit(as.character(x)," About "))[1],collapse = " ")}
))

# Format county Column
sah$county = unlist(lapply(sah$county,function(x){
  paste(unlist(strsplit(as.character(x)," About "))[1],collapse = " ")}
))

# Format Order column
sah$order_type = unlist(lapply(sah$order,function(x){
  paste(unlist(strsplit(as.character(x),", effective "))[1],collapse = "")}
))

# split date-time
sah$date_time = unlist(lapply(sah$order,function(x){
  paste(unlist(strsplit(as.character(x),", effective "))[2],collapse = "")}
))

# time
sah$time = unlist(lapply(sah$date,function(x){
  paste(unlist(strsplit(as.character(x)," at "))[2],collapse = "")}
))

# date
sah$date <- unlist(lapply(sah$date,function(x){
  paste(unlist(strsplit(as.character(x)," at "))[1],collapse = "")}
))

# filter columns
sah <- sah[c("state", "county", "order_type", "date", "time")]
```
