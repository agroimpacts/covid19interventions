---
title: "COVID-19 Spatial Interventions Project"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{COVID-19 Spatial Interventions Project Overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(covid19interventions)
```

## Goal
Compile and analyze COVID-19 related spatial interventions in the U.S. at the county level

## Data & Methods
The data will be extracted from various sources and added to the COVID-19 US Spatial Response database from UArizona.We plan to ingest data from various sources, including county-level orders issued in response to COVID-19 and tweets posted through verified sources. States worst affected by COVID-19 and counties with the highest population within them will be prioritized in the data collection process. We hope to automate the data extraction process using R. We are exploring natural language processing (NLP) methods to parse government orders. We are also collecting data manually in parallel. 

## Responsibilities
**Priyanka**: Research and automation of the data collection process. Exploring NLP libraries in R to parse orders issued by local governments. <br/>

**Noah**: Research & data automation (ingestion) with Twitter data. Exploring TwitteR package to compile a list of verified accounts for counties or public health departments.<br/>

**Priscilla**: Research & analysis<br/>

**Max**: Research & analysis<br/>

## Timeline
- **Week 1**: Identify data sources, investigate methods, create plan
- **Week 2**: Write scripts to extract data, QA/QC data 
- **Week 3**: Analyze/vizualize data and combine data into unifed COVID-19 accessible data base
